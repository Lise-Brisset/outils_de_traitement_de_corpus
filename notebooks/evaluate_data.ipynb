{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44f448c-d3e1-405b-80ec-cde84203c348",
   "metadata": {},
   "source": [
    "# 1. Evaluation du corpus\n",
    "## a. Métriques adapatées\n",
    "\n",
    "Afin d'évaluer les résultats d'un modèle avec notre corpus, nous avons regardé quelles sont les métriques d'évaluations utilisées dans le corpus de référence.\n",
    "\n",
    "La métrique d'évaluation utilisée dans le corpus SQuaD est principalement le __F1-Score__. Une autre métrique d'évaluation en tâche de Q/A est l'__Exact Match__, qui indique s'il y a une correspondance exact entre ce qui est prédit et la vraie réponse.\n",
    "\n",
    "Des explications sur la manière d'importer la métrique d'évaluation officielle de SQuaD se trouve à la page suivante : https://github.com/huggingface/evaluate/tree/main/metrics/squad ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44eaf1f-bebd-45e8-bb22-5e2065df23c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     10\u001b[0m references \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Les prédictions et les références doivent avoir les formes suivantes : \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# On calcule le résultat de F1-Score entre nos prédictions et nos références :\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43msquad_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Affichage des résultats : \u001b[39;00m\n\u001b[1;32m     20\u001b[0m result\n",
      "File \u001b[0;32m~/Documents/TAL_M1S2/outil_de_traitement_de_corpus/venv/lib/python3.11/site-packages/evaluate/module.py:467\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {input_name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed):\n\u001b[0;32m--> 467\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/squad.py:110\u001b[0m, in \u001b[0;36mSquad._compute\u001b[0;34m(self, predictions, references)\u001b[0m\n\u001b[1;32m     94\u001b[0m pred_dict \u001b[38;5;241m=\u001b[39m {prediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]: prediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m predictions}\n\u001b[1;32m     95\u001b[0m dataset \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     96\u001b[0m     {\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparagraphs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     }\n\u001b[1;32m    109\u001b[0m ]\n\u001b[0;32m--> 110\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/compute_score.py:70\u001b[0m, in \u001b[0;36mcompute_score\u001b[0;34m(dataset, predictions)\u001b[0m\n\u001b[1;32m     67\u001b[0m             exact_match \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n\u001b[1;32m     68\u001b[0m             f1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n\u001b[0;32m---> 70\u001b[0m exact_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexact_match\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\n\u001b[1;32m     71\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100.0\u001b[39m \u001b[38;5;241m*\u001b[39m f1 \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexact_match\u001b[39m\u001b[38;5;124m\"\u001b[39m: exact_match, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1}\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "#!pip install evaluate\n",
    "from evaluate import load\n",
    "\n",
    "# on importe la métrique de squad\n",
    "squad_metric = load(\"squad\")\n",
    "\n",
    "# Nous avons d'une part la liste des prédictions données par un modèle :\n",
    "predictions = []\n",
    "# D'autre part la liste des réponses de références, dans le même ordre :\n",
    "references = []\n",
    "\n",
    "# Les prédictions et les références doivent avoir les formes suivantes :\n",
    "# predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n",
    "# references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
    "\n",
    "# On calcule le résultat de F1-Score entre nos prédictions et nos références :\n",
    "results = squad_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Affichage des résultats :\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75616dd3-d8a4-4dea-a74a-9d40d134282f",
   "metadata": {},
   "source": [
    "## b. Métriques proposées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7853e16d-20f1-49fb-a3b1-b2610cf51ac6",
   "metadata": {},
   "source": [
    "#### ROUGE : \n",
    "\n",
    "Nous pourrions utiliser la métrique d'évaluation __ROUGE (Recall-Oriented Understudy for Gisting Evaluation)__. \n",
    "Cette métrique est généralement utilisée dans des tâches de résumé automatique et de traduction automatique. Cependant, cette métrique d'évaluation permet de comparer, dans le cas d'un résumé automatique, le taux de similarité entre le résumé de référence et le résumé produit. \n",
    "Nous pourrions ainsi comparer, dans le cas de notre tâche de Q/A, la réponse donnée par rapport à la réponse de référence.\n",
    "\n",
    "#### WER : \n",
    "Une autre possibilité pourrait être de calculer le taux d'erreur mot. Il eput nous permettre de calculer le taux de mots non attendu dans la réponse, qui sont donc concidérés comme des erreurs.\n",
    "\n",
    "Le calcul du Word Error Rate est le suivant : __WER = (S + D + I) / N__\n",
    "- S : nombre de substitutions\n",
    "- D : nombre de délétions\n",
    "- I : nombre d'insertions\n",
    "\n",
    "Exemple : \n",
    "- question : _\"Que témoigne le sourire ?\"_, vraie réponse : _\"la sympathie\"_, réponse produite : _\"le plaisir\"_\n",
    "--> 2/2 de __S__, WER = 100%\n",
    "\n",
    "- question : _\"Que témoigne le sourire ?\"_, vraie réponse : _\"la sympathie\"_, réponse produite : _\"sympathie\"_\n",
    "--> 1/2 de __D__, WER = 50%\n",
    "\n",
    "- question : _\"Que témoigne le sourire ?\"_, vraie réponse : _\"la sympathie\"_, réponse produite : _\"témoignant en général de la sympathie\"_\n",
    "--> 4/2 de __I__, WER = 100%\n",
    "\n",
    "- question : _\"Que témoigne le sourire ?\"_, vraie réponse : _\"la sympathie\"_, réponse produite : _\"la sympathie\"_\n",
    "--> WER = 0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43de854d-5fb7-46d8-a2ab-ca325addfe0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le taux d'erreur mot (WER) entre ' la sympathie ' et ' sympathie ' est : 0.5\n"
     ]
    }
   ],
   "source": [
    "# site inspiration code : https://thepythoncode.com/article/calculate-word-error-rate-in-python\n",
    "\n",
    "\n",
    "def calcul_wer(reference: str, prediction: str):\n",
    "    \"\"\"\n",
    "    Fonction qui calcule le taux d'erreur mot entre une référence et une prédiction.\n",
    "    Parameters\n",
    "    ----------\n",
    "    reference : str\n",
    "        la réponse de référence du corpus\n",
    "    prediction : str\n",
    "        la réponse produite par le modèle\n",
    "    Returns\n",
    "    -------\n",
    "    wer : float\n",
    "        le taux d'erreur mot entre la référence et la prédiction\n",
    "    \"\"\"\n",
    "    # Récupération de liste de mot dans la référence et la prédiciton\n",
    "    ref_words = reference.split()\n",
    "    pred_words = prediction.split()\n",
    "\n",
    "    # On compte le nombre de substitutions, délétions et insertions.\n",
    "    substitutions = sum(1 for ref, pred in zip(ref_words, pred_words) if ref != pred)\n",
    "    deletions = len(ref_words) - len(pred_words)\n",
    "    insertions = len(pred_words) - len(ref_words)\n",
    "\n",
    "    # Calcul du combre total de mots dans la référence\n",
    "    total_words = len(ref_words)\n",
    "\n",
    "    # Calcul du taux d'erreur mot :\n",
    "    wer = (substitutions + deletions + insertions) / total_words\n",
    "\n",
    "    return wer\n",
    "\n",
    "\n",
    "reference_text = \"la sympathie\"\n",
    "prediction_text = \"sympathie\"\n",
    "\n",
    "wer_score = calcul_wer(reference_text, prediction_text)\n",
    "print(\n",
    "    \"Le taux d'erreur mot (WER) entre '\",\n",
    "    reference_text,\n",
    "    \"' et '\",\n",
    "    prediction_text,\n",
    "    \"' est :\",\n",
    "    wer_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dfc4acb-020f-436c-b1bb-d2dbe31555b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imaginons que nous avons la liste des réponses produites par le modèle\n",
    "# et la liste des vraies réponses de notre corpus\n",
    "# les deux listes ci-dessous sont vides, comme nous ne lançons aucun modèle\n",
    "\n",
    "# liste des vraies réponses, qu'on récupère de notre corpus :\n",
    "reponses_references = []\n",
    "\n",
    "# liste des réponses produites par le modèle :\n",
    "reponses_predites = []\n",
    "\n",
    "# on applique notre fonction de calcul du WER à l'ensemble des réponses :\n",
    "for i in range(len(reponses_references)):\n",
    "    calcul_wer(reponses_references[i], reponses_predites[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9395f8f-7cbd-4641-937d-726157aa0a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forces de  tension superficielle',\n",
       " 'Pseudaelurus',\n",
       " 'la sympathie',\n",
       " '',\n",
       " 'en 1850',\n",
       " 'la  ceinture scapulaire',\n",
       " '50% des poissons vivraient dans 17% de la surface des océans ',\n",
       " 'un tissu épithélial',\n",
       " 'sa présence, de sa beauté, de sa jovialité ou encore pour ses talents',\n",
       " 'miaou',\n",
       " 'campagne',\n",
       " 'forme solide ou liquide',\n",
       " 'Le chat',\n",
       " 'entre l’ œsophage et le duodénum',\n",
       " 'des sons',\n",
       " '',\n",
       " 'le contact',\n",
       " 'un handicap auditif',\n",
       " 'la phase du sommeil au cours de laquelle les  rêves  dont on se souvient se produisent',\n",
       " 'du latin species , «type» ou «apparence»',\n",
       " 'de taille moyenne ou petite',\n",
       " 'des noms scientifiques différents pour désigner un même taxon',\n",
       " 'Antiquité tardive',\n",
       " 'sous la surface intérieure du nez',\n",
       " 'la pureté de la race',\n",
       " 'rayonnements lumineux',\n",
       " '',\n",
       " 'des glandes exocrines',\n",
       " 'éthologique',\n",
       " 'en 1894',\n",
       " '',\n",
       " 'la transpiration',\n",
       " 'en crocs',\n",
       " 'la mélanine et la phéomélanine',\n",
       " 'les  fossiles , les  organismes   asexués  ou pour des espèces rares ou difficiles à observer',\n",
       " 'la  gériatrie',\n",
       " 'certains  mammifères , des  oiseaux  et de certains  reptiles',\n",
       " 'une émasculation',\n",
       " '',\n",
       " 'à une  centrale inertielle',\n",
       " '',\n",
       " '',\n",
       " 'ronron',\n",
       " 'une variété linguistique spécifique à un groupe social ou socio-professionnel, qui permet de se différencier des non-initiés et de se créer une identité propre en utilisant des termes cryptiques',\n",
       " 'les articles qui ont été reconnus comme tels par un vote de la communauté',\n",
       " 'un lundi']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Illustration de comment récupérer l'ensemble des réponses de notre corpus :\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "corpus = pd.read_csv(\"../data/clean/clean_datas_chat.csv\")\n",
    "\n",
    "reponses = list(corpus.answer)\n",
    "\n",
    "reponses_references = []\n",
    "\n",
    "for reference in reponses:\n",
    "    liste_reponse = re.findall(r\"{'text': '(.+?)', 'answer_start':.+\", reference)\n",
    "    if len(liste_reponse) != 0:\n",
    "        reponse = liste_reponse[0]\n",
    "    else:\n",
    "        reponse = \"\"\n",
    "    reponses_references.append(reponse)\n",
    "\n",
    "reponses_references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
